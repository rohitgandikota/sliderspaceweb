<!doctype html>
<html lang="en">
<head>
<title>SliderSpace: Decomposing the Visual Capabilities of Diffusion Models</title>
<meta name="viewport" content="width=device-width,initial-scale=1" />
<meta name="description" content="Automatically decomposing diffusion models' visual capabilities into intuitive slider controls for creative exploration" />
<meta property="og:title" content="SliderSpace: Decomposing the Visual Capabilities of Diffusion Models" />
<meta property="og:description" content="Automatically decomposing diffusion models' visual capabilities into intuitive slider controls for creative exploration" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="twitter:title" content="SliderSpace: Decomposing the Visual Capabilities of Diffusion Models" />
<meta name="twitter:description" content="Automatically decomposing diffusion models' visual capabilities into intuitive slider controls for creative exploration" />
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+Math&display=swap" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

<style>
.relatedthumb {
  float:left; width: 200px; margin: 3px 10px 7px 0;
}
.relatedblock {
  clear: both;
  display: inline-block;
}
.bold-sc {
  font-variant: small-caps;
  font-weight: bold;
}
.cite, .citegroup {
  margin-bottom: 8px;
}
:target {
  background-color: yellow;
}
</style>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FD12LWN557"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date()); gtag('config', 'G-FD12LWN557');
</script>

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
     
<h1 class="lead">
 SliderSpace:
 <nobr class="widenobr">Decomposing the</nobr>
 <nobr class="widenobr">Visual Capabilities of Diffusion Models</nobr>
 </h1>
     
<address>
  <nobr><a href="https://rohitgandikota.github.io/" target="_blank"
  >Rohit Gandikota</a><sup>1</sup>,</nobr>
  <nobr><a href="https://scholar.google.com/citations?user=V8FwQGkAAAAJ&hl=en" target="_blank"
  >Zongze Wu</a><sup>2</sup>,</nobr>
  <nobr><a href="https://richzhang.github.io/" target="_blank"
  >Richard Zhang</a><sup>2</sup>,</nobr>
  <nobr><a href="https://baulab.info/" target="_blank"
  >David Bau</a><sup>1</sup>,</nobr>
  <nobr><a href="https://research.adobe.com/person/eli-shechtman/" target="_blank"
  >Eli Shechtman</a><sup>2</sup>,</nobr>
  <nobr><a href="https://home.ttic.edu/~nickkolkin/home.html" target="_blank"
  >Nick Kolkin</a><sup>2</sup></nobr>
 <br>
  <nobr><sup>1</sup><a href="https://khoury.northeastern.edu/" target="_blank"
  >Northeastern University</a>,</nobr>
  <nobr><sup>2</sup><a href="https://research.adobe.com" target="_blank"
  >Adobe Research</a></nobr>

</address>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row justify-content-center text-center">

<p>
<a href="https://arxiv.org/pdf/2502.01639" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/paper-thumb.png" style="border:1px solid; margin: 0 38px;" alt="ArXiv Preprint thumbnail" data-nothumb=""><br>ArXiv<br>Preprint</a>
    
<a href="https://sliderspace.baulab.info" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/code-thumb.png" style="border:1px solid; margin: 0 38px;" alt="Github code thumbnail" data-nothumb=""><br>Source Code<br>Github (Coming Soon)</a>
    
<a href="https://sliderspace.baulab.info" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/data-thumb.png" style="border:1px solid; margin: 0 38px;" alt="Data thumbnail" data-nothumb=""><br>Discovered<br>Slider Weights (Coming Soon)</a>
    <!-- 
<a href="https://huggingface.co/collections/baulab/elm-6715d68576da0cd1a89c0c04" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/hf-models-thumb.png" style="border:1px solid; margin: 0 38px;" alt="HF Models thumbnail" data-nothumb=""><br>Huggingface<br>ELM Models</a> -->
<!-- <a href="https://unified.baulab.info/weights/uce_models/" class="d-inline-block p-3 align-top" target="_blank"><img height="100" width="78" src="images/data-thumb.png" style="border:1px solid; margin: 0 38px;" alt="Data thumbnail" data-nothumb=""><br>Fine-Tuned<br>Model  Weights</a> -->
<!-- <a href="https://huggingface.co/spaces/baulab/Erasing-Concepts-In-Diffusion" class="d-inline-block p-3 align-bottom" target="_blank"><img height="78" width="104" src="images/demo3x4-thumb.png" style="border:1px solid" alt="Huggingface demo thumbnail" data-nothumb=""><br>Huggingface<br>Demo</a> -->
</p>

<figure class="center_image" style="margin-top: 30px">
<center><img src="sliderspace_gifs/twitter_teaser_website.gif" style="width:50%; max-width:300px"></center>
    <figcaption> SliderSpace reveals maps of the visual knowledge naturally encoded within diffusion models. It works by decomposing the model's capabilities into intuitive, composable sliders.
    </figcaption>
</figure>


<div class="card" style="max-width: 1020px;">
<div class="card-block">
<h3>How do we make diffusion models more controllable and interpretable?</h3>
<p>
<p>
Text-to-image diffusion models can create infinite diverse images from a single prompt, but we don't really understand how they organize their creative knowledge. Until now, users had to discover interesting creative variations through trial and error - tweaking text descriptions, combining different styles, or referencing other images. This process relies heavily on user creativity rather than understanding what the model actually knows about different concepts.
</p>
<p>
We introduce SliderSpace - a way to unlock the creative potential of diffusion models. Instead of requiring users to find creative directions, SliderSpace automatically discovers them from the model's knowledge. Given a concept prompt like "toy", SliderSpace identifies the key visual variations the model knows about it and turns them into simple sliders. No need to tell it what to look for - SliderSpace finds these creative controls on its own.
</p>
</div><!--card-block-->
</div><!--card-->

</div><!--row-->
  
<div class="row">
<div class="col">

    <figure class="center_image" style="margin-top: 30px">
  <center><img src="images/paper/intro.png" style="width:100%; max-width:900px"></center>
  <figcaption>Given a single prompt, SliderSpace can automatically discover creative direction from the diffusion model's knowledge. Unlike previous methods where users have  to creative to get interesting sliders, SliderSpace extracts the model's creativity and provides the directions as sliders for users to control
  </figcaption>
</figure>
  


  <h2> Why discover directions automatically? </h2>
  <p>Recent diffusion models learn from billions of text-images pairs and gain rich knowledge about different visual concepts. Having users manually specify control directions - like "make it more red" or "make it more realistic" - only scratches the surface of what these models understand. It's similar to trying to organize a massive library by only looking at book covers - you'll miss many important categories and connections that exist within. </p>
    <p>By letting the model reveal its own understanding, SliderSpace uncovers creative possibilities that might not be obvious to humans. For example, when exploring the concept "monster", SliderSpace discovers not just obvious controls like size and color, but also nuanced variations in creature anatomy, texture patterns, and environmental elements that we might not think to specify manually. This automated discovery helps us better utilize the full creative potential that exists within these AI models.</p>
  
<h2> How does SliderSpace discover meaningful directions? </h2>
  <figure class="center_image" style="margin-top: 30px">
    <center><img src="images/paper/method.png" style="width:100%; max-width:900px"></center>
    <figcaption> Given a prompt, SliderSpace first generates images and extracts the CLIP features. We then compute the principle directions of variations in CLIP features and align the each slider with extracted principle components. Each slider is therefore trained to represent a unique semantic direction that are relevant in the diffusion model's knowledge of the prompt.
    </figcaption>
  </figure>

    <p>For a given prompt, diffusion model can generate an endless variety of images. SliderSpace uses this capability to understand how the diffusion model thinks about that prompt. Think of it as studying hundreds of drawings to understand an artist's different interpretations of the same concept. </p>
<p>To capture this knowledge systematically, SliderSpace uses low rank adapters called sliders. However, finding meaningful variations isn't straightforward - we need a way to identify changes that make sense to humans. This is why SliderSpace uses CLIP, an contrastive model that understands images in ways that is explainable through natural language. By analyzing diffusion models' variations through CLIP's understanding, SliderSpace ensures the discovered controls are meaningful and intuitive. SliderSpace decomposes a concept based on 3 key desiderata:</p>
    <ul>
        <li class="mb-2"><strong>Unsupervised Discovery:</strong> Directions emerge naturally from the model's learned manifold without imposing predefined attributes</li>
        <li class="mb-2"><strong>Semantic Orthogonality:</strong> Each discovered direction represents a distinct semantic aspect, preventing redundant controls</li>
        <li class="mb-2"><strong>Distribution Consistency:</strong>Transformations by a discovered direction remain consistent across different random seeds and prompt variations</li>
    </ul>
<p>To be precise, SliderSpace first generates a few variations of images from the user defined prompt. It then captures the CLIP features for each individual image and decomposes the directions through PCA. These PCA directions capture the prinicple semantic directions of variations in the diffusion model's knowledge. Once the directions are discovered, SliderSpace trains a slider to capture each individual direction. This way, a slider would steer the diffusion model towards a semantic direction (e.g. "lego" direction for the concept "toy").</p>
    
<h2>How is SliderSpace useful?</h2>
    <p>SliderSpace can be used for decomposing knowledge of diffusion model about a concept. These directions can be used not only for controlling the diffusion model's outputs but also as an interpretability tool to explore the knowledge of a concept. For example, users can explore the concept of art in a pretrained diffusion model or explore the concept of "diversity".</p>

    <h3>Concept Decomposition</h3>
      <p>Given a concept like "monster", SliderSpace reveals fascinating insights into how the model think. The discovered controls went far beyond basic attributes, uncovering dimensions like creature anatomy, environmental context, and artistic interpretation. Our measurements showed that images created using these SliderSpace controls were 40% more diverse than base model's generations.</p>
        
      <figure class="center_image" style="margin-top: 30px">
        <center><img src="images/paper/concept.png" style="width:100%; max-width:800px"></center>
        <figcaption> Erasing 1 artist. On left, we show the original vs edited for the intended erasure. On the right, we show the interference with unerased artists. We find that our method efficiently erases while having the least inteference.
        </figcaption>
      </figure>

    We show 3 random sliderspace sliders for the concept "dragon". We slide the scales from 0 to 1 and show the effect:
    <div style="display: flex; justify-content: space-between; gap: 20px; margin-top: 30px">
      <figure class="center_image" style="margin: 0; flex: 1">
        <img src="sliderspace_gifs/dragon/black_metal_dragon_slider_16_21546.gif" style="width: 100%; max-width: 300px">
        <figcaption style="text-align: center">Black Metal Dragon</figcaption>
      </figure>
      
    
      <figure class="center_image" style="margin: 0; flex: 1">
        <img src="sliderspace_gifs/dragon/purple_dragon_slider_25_392.gif" style="width: 100%; max-width: 300px">
        <figcaption style="text-align: center">Purple Dragon</figcaption>
      </figure>

        <figure class="center_image" style="margin: 0; flex: 1">
        <img src="sliderspace_gifs/dragon/tradional_red_dragon_slider_20_21546.gif" style="width: 100%; max-width: 300px">
        <figcaption style="text-align: center">Traditional Red Dragon</figcaption>
      </figure>
    </div>

    <h3>Exploring Artistic Knowledge of the Model</h3>
        <p>Instead of relying on artist names or style descriptions, SliderSpace automatically can map out how diffusion models understand artistic style. By analyzing "artwork in the style of a famous artist", it discovered distinct artistic directions. These automatically discovered controls proved remarkably effective - they closely match the diversity of manually curated artist lists that took months to compile. In fact, when compared against a dataset of 4,388 artist styles, SliderSpace achieved comparable diversity with far fewer controls.</p>
        <p>Our user studies validated this: 72% of participants found SliderSpace's style controls more useful than traditional methods.</p>
        <figure class="center_image" style="margin-top: 30px">
            <center><img src="images/paper/art.png" style="width:100%; max-width:800px"></center>
            <figcaption> Erasing 1 artist. On left, we show the original vs edited for the intended erasure. On the right, we show the interference with unerased artists. We find that our method efficiently erases while having the least inteference.
            </figcaption>
          </figure>

    <h3>Improving Diversity of a Model</h3>
        <p>We discovered SliderSpace for generic prompts from COCO-30K on SDXL-DMD, a fast 4-step distilled model that typically produces less diverse images than its base SDXL version. SliderSpace improved the diversity score (FID) from 15.52 to 12.12 (lower is better), nearly matching the original model's score of 11.72. This means users can enjoy both fast generation and rich variety. Importantly, these improvements worked across different diffusion architectures, including SDXL, SDXL-Turbo, and FLUX Schnell, showing the method's broad applicability.</p>
        <figure class="center_image" style="margin-top: 30px">
            <center><img src="images/paper/diverse.png" style="width:100%; max-width:800px"></center>
            <figcaption> Erasing 1 artist. On left, we show the original vs edited for the intended erasure. On the right, we show the interference with unerased artists. We find that our method efficiently erases while having the least inteference.
            </figcaption>
          </figure>
  
<h2>How to cite</h2>

<p>The paper can be cited as follows.
</p>

<div class="card">
<h3 class="card-header">bibliography</h3>
<div class="card-block">
<p style="text-indent: -3em; margin-left: 3em;" class="card-text clickselect">
Rohit Gandikota, Zongze Wu, Richard Zhang, David Bau, Eli Shechtman, Nick Kolkin. "<em>SliderSpace: Decomposing the Visual Capabilities of Diffusion Models.</em>"
arXiv preprint arXiv:2502.01639 (2025).</nobr>
</p>
</div>
<h3 class="card-header">bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">
@article{gandikota2025sliderspace,
  title={SliderSpace: Decomposing the Visual Capabilities of Diffusion Models},
  author={Rohit Gandikota and Zongze Wu and Richard Zhang and David Bau and Eli Shechtman and Nick Kolkin},
  journal={arXiv preprint arXiv:2502.01639},
  year={2025}
}
</pre>
</div>
</div>
</p>

</div>
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://baulab.info/">About the Bau Lab</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
</script>
</html>

